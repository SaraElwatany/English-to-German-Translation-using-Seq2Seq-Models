{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7644b2",
   "metadata": {
    "id": "9d7644b2"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d05e0",
   "metadata": {
    "id": "5d4d05e0"
   },
   "source": [
    "1. choose a dataset\n",
    "2. download and check the shape of the dataset\n",
    "4. clean the data if needed (hint: regex)\n",
    "5. decide the tokenization strategy (word , character , sub character)\n",
    "6. build the vocab\n",
    "7. build a wrapper around the dataset\n",
    "8. dataloader -> train[0] (input,label)\n",
    "9. Build network architecture\n",
    "10. Training loop\n",
    "11. evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LNTlD2U-GeST",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNTlD2U-GeST",
    "outputId": "14e191f9-6da6-4f05-923e-46c0535f0d1c"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1yGP8qaR6Joj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1yGP8qaR6Joj",
    "outputId": "542fef5b-3605-4f50-d261-5da5c7ae866e"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade datasets fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94398a55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94398a55",
    "outputId": "44a1d35c-d399-456b-d053-1d17091fe7e0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from matplotlib.pyplot import imshow\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "download_result = nltk.download('punkt_tab')\n",
    "print(\"Download success:\", download_result)\n",
    "\n",
    "\n",
    "# For loading datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SEbdnaEeJyN8",
   "metadata": {
    "id": "SEbdnaEeJyN8"
   },
   "source": [
    "### Data Loading & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nltC_vtqHMiv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "6657b54537ff4da99513d6145ebf3703",
      "eeddac5a8e0b43ac8a25d4390d37c619",
      "4754d358db774f2c88171e0e48e69aee",
      "79beb19653844ca5b73551eb3d645cc4",
      "9b3488c2d99349078c156c18f86ebe02",
      "30cd333b3dfe4cab8f2cccb2a1e01e15",
      "437a720fb77148cbb0f6aae75267d24e",
      "07c034b8a58b46ae94493eba6524b651",
      "442b1d74d0b6497fade83811dab0a0c6",
      "994c657371ca4d89a33647add7acde53",
      "6e88cd3c758f432784d71499173a462d",
      "28fcc48ca6d44348965bfb3f31a32441",
      "7c4eb7bda2f446548628aebf36676cb5",
      "0e130c70946c4ccbb2b54806d57c3497",
      "16ba4c58d23a4f43ba534d806c391a64",
      "c2652d4179f5453487c19392a775caaa",
      "dbcf1aaaef214f229fb1dbc2e3264573",
      "d0feddf1825e45ba99c351880e631fb2",
      "f974540f7ae948e28f1fce934126c6ca",
      "3027af6790ed4cc69f0c73cc7135fcfc",
      "528fce5d61e64c9381d8ee94119eadd7",
      "01deca7a306a475b8d14aee5fcc12fd5",
      "84e24f0d91274083b2508e9c02039c06",
      "d94ab9668d12416d9db90c1ef15728e2",
      "a2f2cc8281bf4c74bd12826f6db6e584",
      "672ac0bdcfd144e5889c70ba39d39749",
      "7f2a45467a1d4defa2e926e496c76a7e",
      "db189cbff2e4492bacde11002e277513",
      "26261b2b4f2d4fcb9df7034e67a4c161",
      "dde7ecce6af04a30942497b3cb340cb9",
      "52670ca640fc46948ec6ae600b11a955",
      "fafaf07cafce44e5b9fdf64805cf8b51",
      "d85e34272e8349d1833dc5a95a0bd6c8",
      "74d1d6b532b24dd7a33450e1f0a6dcfb",
      "5af02e8efa224a0c90fcee84f1f35297",
      "2441489cb3714050a217cc916a923fca",
      "6d29484c10534369aaa9129e37d205f2",
      "67b710e44dba4b8b9c19f3f12eff2c82",
      "c58e6273da904569ac9dbb764e5be56f",
      "0745a33b6fab49218b6d3e25794acd1d",
      "4c9694a7433040769aeff124abb1a126",
      "0f079af8e6ec40a292eb97d8ea824467",
      "4f62bc78b2464f26a973558ca7516c4c",
      "9c0f4503eed24cf6804c7ab0bc7aa0f0",
      "8314a9f7a94a42f785ea7552857dd4bb",
      "2b9a6f883918427fbc29f2b1866dd151",
      "eeb41f0bd0994a15a7ee418aaabfe952",
      "94326fbe9e6747a691936508375d1f69",
      "8d703a1030a04642a5d44b1a2e794b00",
      "23bc974bea1744488e68b45c3218383a",
      "a0e8c99a3931474fa6a6c9668659a747",
      "295099c9271645a1aae79cb18a05f4ca",
      "5b15cf1e17e443d0b414df60e334cf5f",
      "3eba9ec346d64eb18749fe9f6137fbd7",
      "27e5c382c9e7496ba0a9b24c37b381b9",
      "65a24e218a3e4a66a33e06f74c2b1ce3",
      "1f0416b534d94d01bcc5e165e1bfbfd8",
      "234846dae08a4025b31abb8222be9986",
      "9573572e5ffe465f98302af1de166059",
      "535991cb4c884bb7bc350f63ab1d786b",
      "c8a448c3ecca4bdd957714cdd666d702",
      "9aa5ddf9aec744d184522f520326fa60",
      "7e2999f05b48454abb68de9779adee38",
      "a8b7683104564cacbe1183e09154dedb",
      "88784c5485dd44b6a37227b412cc7bb7",
      "325f1da012e14dacb89aea923889ec8d"
     ]
    },
    "id": "nltC_vtqHMiv",
    "outputId": "c0aa134b-fa01-4bdf-e0e7-029fb06fd52c"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"iwslt2017\", \"iwslt2017-en-de\", trust_remote_code=True)\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q-AImBbTPdmb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-AImBbTPdmb",
    "outputId": "95c009f6-ffdd-42cc-ed6f-e8fc4d6f94bd"
   },
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "print(f\"Dataset loaded! Total training examples: {len(train_data)}\")\n",
    "print(f'Sample (english & deutch): {train_data[0][\"translation\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IXbQm_PWKZEe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXbQm_PWKZEe",
    "outputId": "66196632-a11b-4ab4-c9a5-401745592c6b"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and tokenize the text\"\"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "# Process training data\n",
    "train_text = []\n",
    "train_labels = []\n",
    "\n",
    "\n",
    "for sample in tqdm(train_data):\n",
    "\n",
    "    # Preprocess Targets\n",
    "    target_tokens = preprocess_text(sample['translation']['de'])\n",
    "    train_labels.append(target_tokens)\n",
    "\n",
    "    # Preprocess text\n",
    "    tokens = preprocess_text(sample['translation']['en'])\n",
    "    train_text.append(tokens)\n",
    "\n",
    "\n",
    "\n",
    "# Process test data\n",
    "test_text = []\n",
    "test_labels = []\n",
    "\n",
    "for sample in tqdm(test_data):\n",
    "\n",
    "    target_tokens = preprocess_text(sample['translation']['de'])\n",
    "    test_labels.append(target_tokens)\n",
    "\n",
    "    tokens = preprocess_text(sample['translation']['en'])\n",
    "    test_text.append(tokens)\n",
    "\n",
    "\n",
    "print(f\"Data preprocessing complete! Example tokenized review: {train_text[0][:15]}...\")\n",
    "print(f\"Data preprocessing complete! Example tokenized label: {train_labels[0]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rjlpYDfZKZEe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjlpYDfZKZEe",
    "outputId": "d3b4315b-39fc-41db-9ad0-1f9268c4eaed"
   },
   "outputs": [],
   "source": [
    "def build_english_vocab(text, max_words=10000):\n",
    "    \"\"\"Build a vocabulary of the most common words\"\"\"\n",
    "    word_count = Counter()\n",
    "\n",
    "    # Count all words\n",
    "    for sentence in text:\n",
    "        word_count.update(sentence)\n",
    "\n",
    "    # Select most common words\n",
    "    most_common = word_count.most_common(max_words - 2)  # -2 for <UNK> and <PAD>\n",
    "    vocab = {word: idx+2 for idx, (word, _) in enumerate(most_common)}\n",
    "\n",
    "    # Add special tokens\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Build vocabulary from training data\n",
    "english_vocab = build_english_vocab(train_text)\n",
    "vocab_size = len(english_vocab)\n",
    "\n",
    "print(f\"Vocabulary created with {vocab_size} words!\")\n",
    "print(f\"Sample words: {list(english_vocab.items())[:10]}\")\n",
    "\n",
    "# Create a reverse mapping for decoding\n",
    "english_idx_to_word = {idx: word for word, idx in english_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L70PI--KT4jt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L70PI--KT4jt",
    "outputId": "2a50c3e3-fc2c-46bd-8615-fded8aaf1387"
   },
   "outputs": [],
   "source": [
    "def build_german_vocab(text, max_words=10000):\n",
    "    \"\"\"Build a vocabulary of the most common words\"\"\"\n",
    "\n",
    "    word_count = Counter()\n",
    "\n",
    "    # Count all words\n",
    "    for sentence in text:\n",
    "        word_count.update(sentence)\n",
    "\n",
    "    # Select most common words\n",
    "    most_common = word_count.most_common(max_words - 4)  # -4 for <UNK>, <PAD>, '<SOS>', and '<EOS>'\n",
    "    vocab = {word: idx+4 for idx, (word, _) in enumerate(most_common)}\n",
    "\n",
    "    # Add special tokens\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    vocab['<SOS>'] = 2\n",
    "    vocab['<EOS>'] = 3\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Build vocabulary from training data\n",
    "german_vocab = build_english_vocab(train_labels)\n",
    "vocab_size = len(german_vocab)\n",
    "\n",
    "print(f\"Vocabulary created with {vocab_size} words!\")\n",
    "print(f\"Sample words: {list(german_vocab.items())[:10]}\")\n",
    "\n",
    "# Create a reverse mapping for decoding\n",
    "german_idx_to_word = {idx: word for word, idx in german_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8YkbiqJYKZEe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "id": "8YkbiqJYKZEe",
    "outputId": "5eeb8747-9043-4b4e-9f3e-3f72ee066193"
   },
   "outputs": [],
   "source": [
    "# Analyze text lengths to determine optimal max_len\n",
    "sentence_lengths = [len(text) for text in train_text]\n",
    "max_sentence_length = max(sentence_lengths)\n",
    "\n",
    "mean_length = np.mean(sentence_lengths)\n",
    "median_length = np.median(sentence_lengths)\n",
    "p95_length = np.percentile(sentence_lengths, 95)\n",
    "\n",
    "print(f\"Maximum review length: {max_sentence_length}\")\n",
    "print(f\"Mean review length: {mean_length:.2f}\")\n",
    "print(f\"Median review length: {median_length}\")\n",
    "print(f\"95th percentile length: {p95_length}\")\n",
    "\n",
    "# Plotting the distribution of sentence lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(review_lengths, bins=50)\n",
    "plt.axvline(x=max_sentence_length, color='r', linestyle='--', label=f'Max: {max_sentence_length}')\n",
    "plt.axvline(x=p95_length, color='g', linestyle='--', label=f'95th: {p95_length:.0f}')\n",
    "plt.axvline(x=median_length, color='b', linestyle='--', label=f'Median: {median_length}')\n",
    "\n",
    "plt.title(\"Distribution of sentence Lengths\")\n",
    "plt.xlabel(\"Length (number of tokens)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Choose max_len based on 95th percentile to avoid excessive padding\n",
    "# while still covering most sentences without truncation\n",
    "chosen_max_len = int(p95_length)\n",
    "print(f\"Chosen max_len: {chosen_max_len}\")\n",
    "\n",
    "def encode_sentence(sentence, vocab, max_len=None, add_sos_eos=False):\n",
    "    \"\"\"Convert a sentence to a fixed-length sequence of integers\"\"\"\n",
    "\n",
    "    if add_sos_eos:\n",
    "      sentence = ['<SOS>'] + sentence + ['<EOS>']\n",
    "\n",
    "    # Use the chosen max_len from analysis if none provided\n",
    "    if max_len is None:\n",
    "        max_len = chosen_max_len\n",
    "\n",
    "    # Encode words, use <UNK> for unknown words\n",
    "    encoded = [vocab.get(word, vocab['<UNK>']) for word in sentence[:max_len]]\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    if len(encoded) < max_len:\n",
    "        encoded += [vocab['<PAD>']] * (max_len - len(encoded))\n",
    "\n",
    "    return encoded\n",
    "\n",
    "# Encode all sentences using the dynamically determined max_len\n",
    "train_encoded = [encode_sentence(sentence, english_vocab) for sentence in tqdm(train_text)]\n",
    "test_encoded = [encode_sentence(sentence, english_vocab) for sentence in tqdm(test_text)]\n",
    "\n",
    "train_labels_encoded = [encode_sentence(sentence, german_vocab) for sentence in tqdm(train_labels)]\n",
    "test_labels_encoded = [encode_sentence(sentence, german_vocab) for sentence in tqdm(test_labels)]\n",
    "\n",
    "# Convert to tensors\n",
    "train_sequences = torch.tensor(train_encoded, dtype=torch.long)\n",
    "train_labels_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "\n",
    "test_sequences = torch.tensor(test_encoded, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "\n",
    "print(f\"Data encoding complete! Example sequence: {train_sequences[0][:15]}...\")\n",
    "print(f\"All sequences are now of length {chosen_max_len}\")\n",
    "\n",
    "# Calculate how many sentences were truncated\n",
    "truncated_count = sum(1 for length in sentence_lengths if length > chosen_max_len)\n",
    "truncated_percentage = (truncated_count / len(sentence_lengths)) * 100\n",
    "print(f\"Sentences truncated: {truncated_count} ({truncated_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HyqgzAvgKZEf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HyqgzAvgKZEf",
    "outputId": "d2fc0724-4b33-4f1e-d948-ff9ce95b9ead"
   },
   "outputs": [],
   "source": [
    "def encode_sentence(sentence, vocab, max_len=200, add_sos_eos=False):\n",
    "    \"\"\"Convert a sentence to a fixed-length sequence of integers\"\"\"\n",
    "\n",
    "    if add_sos_eos:\n",
    "        sentence = ['<SOS>'] + sentence + ['<EOS>']\n",
    "\n",
    "    # Encode words, use <UNK> for unknown words\n",
    "    encoded = [vocab.get(word, vocab['<UNK>']) for word in sentence[:max_len]]\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    if len(encoded) < max_len:\n",
    "        encoded += [vocab['<PAD>']] * (max_len - len(encoded))\n",
    "\n",
    "    return encoded\n",
    "\n",
    "\n",
    "\n",
    "# Encode all sentences\n",
    "train_encoded = [encode_sentence(sentence, english_vocab) for sentence in tqdm(train_text)]\n",
    "train_labels_encoded = [encode_sentence(sentence, german_vocab, add_sos_eos=True) for sentence in tqdm(train_labels)]\n",
    "\n",
    "test_encoded = [encode_sentence(sentence, english_vocab) for sentence in tqdm(test_text)]\n",
    "test_labels_encoded = [encode_sentence(sentence, german_vocab, add_sos_eos=True) for sentence in tqdm(test_labels)]\n",
    "\n",
    "\n",
    "# Convert to tensors\n",
    "train_sequences = torch.tensor(train_encoded, dtype=torch.long)\n",
    "train_labels_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
    "\n",
    "test_sequences = torch.tensor(test_encoded, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "\n",
    "print(f\"Data encoding complete! Example sequence: {train_sequences[0][:15]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iCvcpIc8KZEf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCvcpIc8KZEf",
    "outputId": "2b88f120-4927-4cac-e9f4-29a232f3d943"
   },
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Create train/validation split\n",
    "train_seq, val_seq, train_labels, val_labels = train_test_split(\n",
    "                                                                  train_sequences, train_labels_tensor, test_size=0.1, random_state=42\n",
    "                                                               )\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TranslationDataset(train_seq, train_labels)\n",
    "val_dataset = TranslationDataset(val_seq, val_labels)\n",
    "test_dataset = TranslationDataset(test_sequences, test_labels_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"DataLoaders created! Training batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2AJgs4Z-hIR_",
   "metadata": {
    "id": "2AJgs4Z-hIR_"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        '''\n",
    "        A Function that takes an english sentence creates an embedding to each word in that sentence, pass these embeddings of each token/word one step at a time to the lstm,\n",
    "        the lstm then output the context vector representing that sentence (of hidden_size dimension).\n",
    "\n",
    "        Args:\n",
    "          src (tensor): The input sequence (the english sentence we want to encode).\n",
    "\n",
    "        Returns:\n",
    "          hidden (tensor): Final Vector that represents the final hidden state after encoding the input sentence (context vector of the final step).\n",
    "          cell (tensor): Final Vector that represents the final cell state after encoding the input sentence (context vector of the final step).\n",
    "        '''\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))                            # [batch_size, src_len, emb_dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14XQeyBYhOMG",
   "metadata": {
    "id": "14XQeyBYhOMG"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "\n",
    "        '''\n",
    "        A Function that takes a german sentence creates an embedding to each word in that sentence, pass these embeddings of each token/word one step at a time to the lstm along with the context vectors from previous states,\n",
    "        the lstm then outputs the vector which would be passed to the linear layers to generate probabilities for each token.\n",
    "\n",
    "        Args:\n",
    "          input (tensor): The input sequence (the german sentence we want to encode).\n",
    "          hidden (tensor): Final Vector that represents the final previous hidden state.\n",
    "          cell (tensor): Final Vector that represents the final previous cell state.\n",
    "\n",
    "        Returns:\n",
    "          prediction (tensor):\n",
    "          hidden (tensor): Final Vector that represents the final hidden state.\n",
    "          cell (tensor): Final Vector that represents the final cell state.\n",
    "        '''\n",
    "\n",
    "        # input: [batch_size] -> need [batch_size, 1] for single token\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(input))  # [batch_size, 1, emb_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # output: [batch_size, 1, hidden_dim]\n",
    "        prediction = self.fc_out(output.squeeze(1))  # [batch_size, output_dim]\n",
    "\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wer8kqxLhTlM",
   "metadata": {
    "id": "Wer8kqxLhTlM"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "\n",
    "        '''\n",
    "        A function that performes translation from english to german using sequence to sequence modelling.\n",
    "\n",
    "        Args:\n",
    "\n",
    "          src (): The english sentence we want to translate.\n",
    "          trg (): The targeted german sentence (ground truth).\n",
    "          teacher_forcing_ratio ():\n",
    "\n",
    "        Returns:\n",
    "          outputs (tensor): The translated german sentence.\n",
    "        '''\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[:, 0]  # first <sos> token\n",
    "\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8o-_aAHBiksW",
   "metadata": {
    "id": "8o-_aAHBiksW"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip, teacher_forcing_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Train the Seq2Seq model for one epoch.\n",
    "    Args:\n",
    "        model: The Seq2Seq model\n",
    "        iterator: DataLoader iterator\n",
    "        optimizer: Optimizer (e.g., Adam)\n",
    "        criterion: Loss function (CrossEntropyLoss)\n",
    "        clip: Gradient clipping value\n",
    "        teacher_forcing_ratio: Probability of using teacher forcing during training\n",
    "    Returns:\n",
    "        epoch_loss: Average loss for the epoch\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for batch in tqdm(iterator, desc=\"Training\"):\n",
    "\n",
    "        # Get batch data (source and target sequences)\n",
    "        src, trg = batch\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "\n",
    "        # output shape: [batch_size, trg_len, trg_vocab_size]\n",
    "        # trg shape: [batch_size, trg_len]\n",
    "\n",
    "        # Flatten the output and target for CrossEntropyLoss\n",
    "        output = output[:, 1:].reshape(-1, output.shape[-1])  # [batch_size * trg_len, output_dim]\n",
    "        trg = trg[:, 1:].reshape(-1)  # [batch_size * trg_len]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        # Predicted token: the one with the highest probability (argmax)\n",
    "        _, predicted = output.max(1)\n",
    "\n",
    "        # Calculate the number of correct predictions\n",
    "        correct = (predicted == trg).float()\n",
    "        accuracy = correct.sum() / len(correct)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the epoch loss and accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += accuracy.item()\n",
    "\n",
    "    # Return average loss and accuracy for the epoch\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7FcAN6fbDMl3",
   "metadata": {
    "id": "7FcAN6fbDMl3"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device, trg_pad_idx):\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for src, trg in dataloader:\n",
    "\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            # Get model outputs\n",
    "            output = model(src, trg, teacher_forcing_ratio=0)  # No teacher forcing during evaluation\n",
    "\n",
    "            # output: [batch_size, trg_len, output_dim]\n",
    "            # trg: [batch_size, trg_len]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            # Compute accuracy\n",
    "            preds = output.argmax(dim=1)\n",
    "            non_pad = trg != trg_pad_idx\n",
    "            correct = (preds == trg) & non_pad\n",
    "            acc = correct.sum().float() / non_pad.sum()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cRlu3pD0iIhj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRlu3pD0iIhj",
    "outputId": "b112c092-1e31-4267-9079-8db64ce52a81"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(english_vocab)       # English vocab size\n",
    "OUTPUT_DIM = len(german_vocab)      # German vocab size\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model4 = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "print(f\"Model created and moved to {device}!\")\n",
    "print(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aUS2L_-epYb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2aUS2L_-epYb",
    "outputId": "b5bdc501-3570-4f2d-8989-a2c76264ca8a"
   },
   "outputs": [],
   "source": [
    "# Cross Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(model4.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Loss function and optimizer defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PqXd9Dw1dwzH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqXd9Dw1dwzH",
    "outputId": "7255b7f8-d468-41fd-9b60-5361b8f69fa6"
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "clip = 1.0            # Gradient clipping value\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # Train\n",
    "    train_loss, train_acc = train(model4, train_loader, optimizer, criterion, clip)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # Evaluate\n",
    "    trg_pad_idx = german_vocab['<PAD>']\n",
    "    val_loss, val_acc = evaluate(model4, val_loader, criterion, device, trg_pad_idx)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i-V9GMrMdwzH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "i-V9GMrMdwzH",
    "outputId": "f932f844-a30d-4be9-c7d9-3af01ec3b7df"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train Acc')\n",
    "plt.plot(val_accs, label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eUqFtEX2dwzH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUqFtEX2dwzH",
    "outputId": "30a4a7d1-bae6-4b7f-a64f-725665b5fdcf"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model4, test_loader, criterion, device, trg_pad_idx)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nF3cQck2utw9",
   "metadata": {
    "id": "nF3cQck2utw9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
